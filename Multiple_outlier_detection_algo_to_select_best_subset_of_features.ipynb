{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17c89a6b",
   "metadata": {},
   "source": [
    "Note: Using outlier detection algorithms to select features can be quite complex, as these methods don't naturally lend themselves to ranking or selecting features. This is a very unconventional approach and may not yield good results, as these algorithms were not designed for feature selection. Be sure to thoroughly test and validate this method before using it for critical tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a063ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyod.models.hbos import HBOS\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from keras.layers import Input, Dense\n",
    "from keras.models import Model\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "# Fit the Isolation Forest model\n",
    "iso = IsolationForest(contamination=0.01, random_state=0)\n",
    "iso.fit(df_scaled)\n",
    "\n",
    "# Get feature importances from Isolation Forest\n",
    "iso_importances = iso.feature_importances_\n",
    "\n",
    "# Fit the Local Outlier Factor model\n",
    "lof = LocalOutlierFactor(novelty=True, contamination=0.01)\n",
    "lof.fit(df_scaled)\n",
    "\n",
    "# Get feature importances from LOF, which we'll define as the negative mean k-nearest neighbors distance\n",
    "lof_importances = -lof.kneighbors()[0].mean(axis=0)\n",
    "\n",
    "# Fit the HBOS model\n",
    "hbos = HBOS(contamination=0.01)\n",
    "hbos.fit(df_scaled)\n",
    "\n",
    "# Get feature importances from HBOS\n",
    "hbos_importances = hbos.decision_scores_\n",
    "\n",
    "# Fit the XGBOD model (Extended Isolation Forest)\n",
    "xgbod = XGBOD(contamination=0.01)\n",
    "xgbod.fit(df_scaled)\n",
    "\n",
    "# Get feature importances from XGBOD\n",
    "xgbod_importances = xgbod.decision_scores_\n",
    "\n",
    "# Fit the Autoencoder model\n",
    "input_layer = Input(shape=(df_scaled.shape[1],))\n",
    "encoded = Dense(64, activation='relu')(input_layer)\n",
    "decoded = Dense(df_scaled.shape[1], activation='sigmoid')(encoded)\n",
    "\n",
    "autoencoder = Model(input_layer, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "autoencoder.fit(df_scaled, df_scaled, epochs=50, batch_size=256, shuffle=True, validation_split=0.2, verbose=0)\n",
    "\n",
    "# Get feature importances from Autoencoder, which we'll define as the reconstruction error\n",
    "predictions = autoencoder.predict(df_scaled)\n",
    "mse = np.mean(np.power(df_scaled - predictions, 2), axis=1)\n",
    "autoencoder_importances = mse.values\n",
    "\n",
    "# Combine importances\n",
    "average_importances = (iso_importances + lof_importances + hbos_importances + xgbod_importances + autoencoder_importances) / 5\n",
    "\n",
    "# Define the number of top features to select\n",
    "N = 10\n",
    "\n",
    "# Get the top N features\n",
    "top_N_features = df.columns[average_importances.argsort()[-N:]]\n",
    "\n",
    "# Select these features from the original dataframe\n",
    "df_reduced = df[top_N_features]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
