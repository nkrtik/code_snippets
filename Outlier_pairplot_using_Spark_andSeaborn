from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, lit, percentile_approx

# Initialize Spark session
spark = SparkSession.builder.appName("IrisOutliers").getOrCreate()

# Load the Iris dataset into a Spark DataFrame
# For demonstration purposes, using a small dataset
# Replace this with your large dataset
iris = load_iris()
data = iris.data
columns = iris.feature_names

# Convert to Spark DataFrame
df = spark.createDataFrame(pd.DataFrame(data, columns=columns))

# Calculate the 1st and 99th percentiles for each column
percentiles = df.agg(
    *[percentile_approx(col(c), [0.01, 0.99]).alias(c) for c in columns]
).collect()[0]

# Create lower and upper bounds dictionaries
lower_bounds = {c: percentiles[c][0] for c in columns}
upper_bounds = {c: percentiles[c][1] for c in columns}

# Add a new column 'category' to categorize data
for c in columns:
    df = df.withColumn(
        'category',
        when(col(c) < lower_bounds[c], 'lower_outlier')
        .when(col(c) > upper_bounds[c], 'higher_outlier')
        .otherwise(col('category'))
    )

# Fill null values with 'normal'
df = df.fillna('normal', subset=['category'])

# Convert to Pandas DataFrame for visualization
pandas_df = df.toPandas()


import seaborn as sns
import matplotlib.pyplot as plt

# Initialize the PairGrid with the 'category' column as hue
g = sns.PairGrid(pandas_df, hue="category", diag_sharey=False)
g.map_upper(sns.scatterplot)
g.map_lower(sns.kdeplot)
g.map_diag(sns.kdeplot)
g.map_offdiag(sns.scatterplot)

# Add a legend
g.add_legend()

plt.show()
##################################################################################################################
## Another version using pandas
##################################################################################################################
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

# Calculate percentiles
percentiles = df.quantile([0.05, 0.95])

# Function to categorize each value
def categorize_value(val, lower_bound, upper_bound):
    return 'outlier' if val < lower_bound or val > upper_bound else 'normal'

# Apply categorization to each row and create a new column 'category'
def categorize_row(row):
    for col in df.columns:
        lower_bound = percentiles[col].iloc[0]
        upper_bound = percentiles[col].iloc[1]
        if categorize_value(row[col], lower_bound, upper_bound) == 'outlier':
            return 'outlier'
    return 'normal'

df['category'] = df.apply(categorize_row, axis=1)

# Initialize the PairGrid with the 'category' column as hue
g = sns.PairGrid(df, hue="category", diag_sharey=False)
g.map_upper(sns.scatterplot)
#g.map_diag(sns.histplot, color=".1")  # Uncomment for histograms on diagonal
g.map_lower(sns.kdeplot, color=".3")
g.map_diag(sns.kdeplot)
g.map_offdiag(sns.scatterplot)

# Add a legend
g.add_legend()

plt.show()


