{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de4ac1e3",
   "metadata": {},
   "source": [
    "The code produces a 1% sample of the original dataset that maintains the same distribution of the 'label' column for each date. The following are the steps followed:\n",
    "\n",
    "Initialization: It sets up a PySpark session and defines the columns in the data that are considered features (excluding 'date' and 'id').\n",
    "\n",
    "Outlier Detection:\n",
    "1. For each unique date in the dataset, it filters the data for that date.\n",
    "2. For each feature column, it calculates the first and third quartiles (Q1 and Q3) and then determines the Interquartile Range (IQR). Using these, it identifies the lower and upper bounds for outliers.\n",
    "3. A new column is created for each feature to indicate whether a value is an outlier (1 if yes, 0 if no).\n",
    "4. A combined 'label' column is derived which is set to 1 if any of the features for a given row is an outlier, otherwise 0.\n",
    "5. The intermediate outlier columns for individual features are then dropped.\n",
    "\n",
    "Stratified Sampling:\n",
    "1. For each date, the code calculates the fraction of rows with each label (0 or 1).\n",
    "2. It then determines the number of samples required for each label such that the total sample size is 1% of the entire dataset and the samples are evenly spread across different dates, while maintaining the same proportion of each label as in the original data.\n",
    "3. The data for each date is then sampled based on these calculated fractions.\n",
    "\n",
    "Aggregation: All the sampled dataframes for each date are unioned together to form a final sampled dataframe.\n",
    "Summary Statistics: The code calculates and provides summary statistics for both the entire dataset (population) and the sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b91848d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count, approxQuantile\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"Outlier_labelling\").getOrCreate()\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "# df = spark.read.... # Read your dataframe here\n",
    "\n",
    "feature_columns = [c for c in df.columns if c not in ['date', 'id']]\n",
    "all_dates = df.select(\"date\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "sampled_dfs = []  # Store sampled dataframes for each date\n",
    "\n",
    "# Calculate the desired total sample size and the sample size per date\n",
    "total_count = df.count()\n",
    "desired_sample_count = int(0.01 * total_count)  # 1% of total records\n",
    "desired_count_per_date = desired_sample_count // len(all_dates)\n",
    "\n",
    "for date in all_dates:\n",
    "    temp_df = df.filter(col(\"date\") == date)\n",
    "\n",
    "    # Outlier Detection for each feature column\n",
    "    for column in feature_columns:\n",
    "        Q1, Q3 = temp_df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        temp_df = temp_df.withColumn(\"outlier_{}\".format(column),\n",
    "                                     when((col(column) < lower_bound) | (col(column) > upper_bound), 1).otherwise(0))\n",
    "\n",
    "    # Label column - if any column is an outlier, label as 1, otherwise 0\n",
    "    temp_df = temp_df.withColumn(\"label\", sum(col(\"outlier_{}\".format(c)) for c in feature_columns))\n",
    "\n",
    "    # Drop intermediary outlier columns\n",
    "    for column in feature_columns:\n",
    "        temp_df = temp_df.drop(\"outlier_{}\".format(column))\n",
    "\n",
    "    # Calculate the fractions of each label for the current date\n",
    "    label_counts = temp_df.groupBy(\"label\").count().rdd.collectAsMap()\n",
    "    total_for_date = temp_df.count()\n",
    "    label_fractions = {label: count / total_for_date for label, count in label_counts.items()}\n",
    "\n",
    "    # Calculate the desired number of samples for each label\n",
    "    desired_samples_by_label = {label: int(desired_count_per_date * fraction) for label, fraction in label_fractions.items()}\n",
    "\n",
    "    # Sample the desired number of records for each label\n",
    "    samples_for_date = [temp_df.filter(col(\"label\") == label).sample(withReplacement=False, fraction=min(1.0, desired_samples_by_label[label]/label_counts[label])) for label in label_counts.keys()]\n",
    "    \n",
    "    sampled_temp_df = samples_for_date[0]\n",
    "    for s_df in samples_for_date[1:]:\n",
    "        sampled_temp_df = sampled_temp_df.union(s_df)\n",
    "    \n",
    "    sampled_dfs.append(sampled_temp_df)\n",
    "\n",
    "# Union all sampled dataframes\n",
    "final_sampled_df = sampled_dfs[0]\n",
    "for s_df in sampled_dfs[1:]:\n",
    "    final_sampled_df = final_sampled_df.union(s_df)\n",
    "\n",
    "# Summary Statistics\n",
    "population_summary = df.describe().toPandas()\n",
    "sample_summary = final_sampled_df.describe().toPandas()\n",
    "\n",
    "# Save or display the population and sample summary\n",
    "# population_summary.show()\n",
    "# sample_summary.show()\n",
    "\n",
    "# Rename columns for clarity\n",
    "population_summary.columns = ['Statistic'] + [f'Population_{col}' for col in population_summary.columns[1:]]\n",
    "sample_summary.columns = ['Statistic'] + [f'Sample_{col}' for col in sample_summary.columns[1:]]\n",
    "\n",
    "# Merge the dataframes on 'Statistic' column\n",
    "comparison_df = population_summary.merge(sample_summary, on='Statistic')\n",
    "\n",
    "print(comparison_df)\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
