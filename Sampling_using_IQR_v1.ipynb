{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3b767b1",
   "metadata": {},
   "source": [
    "The code produces a 1% sample of the original dataset that maintains the same distribution of the 'label' column for each date. The following are the steps followed:\n",
    "\n",
    "Initialization: It sets up a PySpark session and defines the columns in the data that are considered features (excluding 'date' and 'id').\n",
    "\n",
    "Outlier Detection:\n",
    "1. For each unique date in the dataset, it filters the data for that date.\n",
    "2. For each feature column, it calculates the first and third quartiles (Q1 and Q3) and then determines the Interquartile Range (IQR). Using these, it identifies the lower and upper bounds for outliers.\n",
    "3. A new column is created for each feature to indicate whether a value is an outlier (1 if yes, 0 if no).\n",
    "4. A combined 'label' column is derived which is set to 1 if any of the features for a given row is an outlier, otherwise 0.\n",
    "5. The intermediate outlier columns for individual features are then dropped.\n",
    "\n",
    "Stratified Sampling:\n",
    "1. For each date, the code calculates the fraction of rows with each label (0 or 1).\n",
    "2. It then determines the number of samples required for each label such that the total sample size is 1% of the entire dataset and the samples are evenly spread across different dates, while maintaining the same proportion of each label as in the original data.\n",
    "3. The data for each date is then sampled based on these calculated fractions.\n",
    "\n",
    "Aggregation: All the sampled dataframes for each date are unioned together to form a final sampled dataframe.\n",
    "Summary Statistics: The code calculates and provides summary statistics for both the entire dataset (population) and the sampled dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696b442",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, functions as F\n",
    "from functools import reduce\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"OutlierDetection\").getOrCreate()\n",
    "\n",
    "# Function to check if a column exists in a DataFrame\n",
    "def column_exists(df, column_name):\n",
    "    return column_name in df.columns\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "# df = spark.read.... # Read your dataframe here\n",
    "\n",
    "feature_columns = [c for c in df.columns if c not in ['date', 'id']]\n",
    "all_dates = [row.date for row in df.select(\"date\").distinct().collect() if row.date is not None]\n",
    "\n",
    "sampled_dfs = []\n",
    "\n",
    "# Calculate the desired total sample size and the sample size per date\n",
    "total_count = df.count()\n",
    "desired_sample_count = int(0.01 * total_count)\n",
    "desired_count_per_date = desired_sample_count // len(all_dates)\n",
    "\n",
    "for date in all_dates:\n",
    "    temp_df = df.filter(F.col(\"date\") == date)\n",
    "\n",
    "    # Outlier Detection for each feature column\n",
    "    for column in feature_columns:\n",
    "        try:\n",
    "            quantiles = temp_df.approxQuantile(column, [0.25, 0.75], 0.01)\n",
    "            Q1, Q3 = quantiles\n",
    "            IQR = Q3 - Q1\n",
    "            lower_bound = Q1 - 1.5 * IQR\n",
    "            upper_bound = Q3 + 1.5 * IQR\n",
    "            \n",
    "            temp_df = temp_df.withColumn(\"outlier_{}\".format(column),\n",
    "                                         F.when((F.col(column) < lower_bound) | (F.col(column) > upper_bound), 1).otherwise(0))\n",
    "        except:\n",
    "            print(f\"Error processing column {column} on date {date}. Skipping outlier detection for this column.\")\n",
    "\n",
    "    # Check if outlier columns were created\n",
    "    for c in feature_columns:\n",
    "        if \"outlier_{}\".format(c) not in temp_df.columns:\n",
    "            print(f\"Outlier column for {c} was not created!\")\n",
    "\n",
    "    # Sum the outlier columns to create the label column\n",
    "    temp_df = temp_df.withColumn(\"label\", reduce(lambda x, y: x + y, [F.col(\"outlier_{}\".format(c)) for c in feature_columns]))\n",
    "\n",
    "    # Drop outlier columns if they exist\n",
    "    for column in feature_columns:\n",
    "        outlier_column_name = \"outlier_{}\".format(column)\n",
    "        if column_exists(temp_df, outlier_column_name):\n",
    "            temp_df = temp_df.drop(outlier_column_name)\n",
    "\n",
    "    label_counts = temp_df.groupBy(\"label\").count().rdd.collectAsMap()\n",
    "    total_for_date = temp_df.count()\n",
    "    label_fractions = {label: count / total_for_date for label, count in label_counts.items()}\n",
    "\n",
    "    samples_for_date = []\n",
    "    for label, fraction in label_fractions.items():\n",
    "        desired_samples = int(desired_count_per_date * fraction)\n",
    "        sample_fraction = min(1.0, desired_samples/label_counts[label])\n",
    "        \n",
    "        if sample_fraction == 1.0 and desired_samples > label_counts[label]:\n",
    "            sampled_data = temp_df.filter(F.col(\"label\") == label).sample(withReplacement=True, fraction=sample_fraction)\n",
    "        else:\n",
    "            sampled_data = temp_df.filter(F.col(\"label\") == label).sample(withReplacement=False, fraction=sample_fraction)\n",
    "        \n",
    "        samples_for_date.append(sampled_data)\n",
    "\n",
    "    sampled_temp_df = samples_for_date[0]\n",
    "    for s_df in samples_for_date[1:]:\n",
    "        sampled_temp_df = sampled_temp_df.union(s_df)\n",
    "    \n",
    "    sampled_dfs.append(sampled_temp_df)\n",
    "\n",
    "final_sampled_df = sampled_dfs[0]\n",
    "for s_df in sampled_dfs[1:]:\n",
    "    final_sampled_df = final_sampled_df.union(s_df)\n",
    "\n",
    "population_summary = df.describe().toPandas()\n",
    "sample_summary = final_sampled_df.describe().toPandas()\n",
    "\n",
    "# Display the population and sample summary\n",
    "print(\"Population Summary:\")\n",
    "print(population_summary)\n",
    "print(\"\\nSample Summary:\")\n",
    "print(sample_summary)\n",
    "\n",
    "# Stop the Spark Session\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
