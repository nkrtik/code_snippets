from pyspark.sql import functions as F
from pyspark.sql import Window

# -----------------------------------------------------------------------------
# 1) Example column definitions (adjust to your actual column variables)
# -----------------------------------------------------------------------------
user_id_col = ["user_id_part1", "user_id_part2", "user_id_part3"]  # e.g. three columns
date_col = ["dt"]  # single date column in a list
ip_col = "src_ip_bc"  # IP address column name

# Suppose your input DataFrame is df with columns:
#   [user_id_part1, user_id_part2, user_id_part3, dt, src_ip_bc, ...]
# Adjust names/types as needed.
# -----------------------------------------------------------------------------

# -----------------------------------------------------------------------------
# 2) Mark each (user, date, IP) row as "new IP" or "old IP"
#    Window partition is by (user_id, ip), ordered by the date.
# -----------------------------------------------------------------------------
# Window to find previous usage of the SAME IP by the SAME user
w_ip = Window.partitionBy(*user_id_col, ip_col).orderBy(F.col(date_col[0]).asc())

df_marked = (
    df
    # previous date of usage for that user+IP
    .withColumn(
        "prev_ip_dt",
        F.lag(date_col[0]).over(w_ip)
    )
    # number of days since this user+IP was last used
    .withColumn(
        "days_since_last_ip",
        F.datediff(F.col(date_col[0]), F.col("prev_ip_dt"))
    )
    # is_new_ip: 1 if gap > 30 or if no previous usage, else 0
    .withColumn(
        "is_new_ip",
        F.when(
            (F.col("days_since_last_ip") > 30) | (F.col("days_since_last_ip").isNull()),
            1
        ).otherwise(0)
    )
)

# -----------------------------------------------------------------------------
# 3) Aggregate daily per user:
#    - Distinct IPs used that day
#    - How many of them are marked as new?
# -----------------------------------------------------------------------------
df_day_agg = (
    df_marked
    # First group by (user, date, ip) in case of duplicates in the same day
    .groupBy(*user_id_col, date_col[0], ip_col)
    .agg(F.max("is_new_ip").alias("is_new_ip"))  # if multiple rows for same IP/day
    # Then group again by user+day to get count of distinct IPs and new IPs
    .groupBy(*user_id_col, date_col[0])
    .agg(
        F.countDistinct(ip_col).alias("count_ip"),
        F.sum("is_new_ip").alias("count_new_ip")
    )
)

# -----------------------------------------------------------------------------
# 4) Check if the user had ANY usage in the last 30 days.
#    If not, we assign category = -1.
#
#    A simple trick:
#    - For each (user, day), look at the immediately PREVIOUS day the user used the system.
#    - If dt - prev_dt <= 30, user had usage in the last 30 days; otherwise, no.
# -----------------------------------------------------------------------------
w_user = Window.partitionBy(*user_id_col).orderBy(F.col(date_col[0]).asc())

df_day_usage = (
    df_day_agg
    # previous day the user used the system at all
    .withColumn(
        "prev_dt",
        F.lag(date_col[0]).over(w_user)
    )
    # has_usage_last_30d: 1 if dt - prev_dt <= 30, else 0 (or if no prev_dt => 0)
    .withColumn(
        "has_usage_last_30d",
        F.when(
            F.col("prev_dt").isNotNull() & (F.datediff(F.col(date_col[0]), F.col("prev_dt")) <= 30),
            1
        ).otherwise(0)
    )
)

# -----------------------------------------------------------------------------
# 5) Compute fraction of new IP addresses for that day -> define the categorical value
#    fraction_new = count_new_ip / count_ip
#    category =
#       -1 if has_usage_last_30d == 0
#        0 if fraction_new == 0.0
#        1 if 0.0 < fraction_new < 1.0
#      100 if fraction_new == 1.0
# -----------------------------------------------------------------------------
df_final = (
    df_day_usage
    .withColumn(
        "fraction_new",
        (F.col("count_new_ip") / F.col("count_ip")).cast("double")
    )
    .withColumn(
        "ip_new_category",
        F.when(
            F.col("has_usage_last_30d") == 0,  # no usage last 30 days
            F.lit(-1)
        ).when(
            F.col("fraction_new") == 0,
            F.lit(0)
        ).when(
            F.col("fraction_new") == 1,
            F.lit(100)
        ).otherwise(F.lit(1))
    )
)

# df_final now has:
#  [ user_id_part1, user_id_part2, user_id_part3, dt,
#    count_ip, count_new_ip, has_usage_last_30d, fraction_new, ip_new_category ]
#
# You can rename columns or select only those you want in the final output.
# -----------------------------------------------------------------------------

# Example final select:
df_result = df_final.select(
    *user_id_col,
    date_col[0],
    "ip_new_category"
)

# df_result is your desired DataFrame with the new categorical feature
# indicating IP-newness compared to the last 30 days.
